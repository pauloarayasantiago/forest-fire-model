{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from skopt import BayesSearchCV\n",
    "# from skopt.space import Real, Integer\n",
    "# from scipy import stats\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'forestfires.csv'\n",
    "# forest_fires_df = pd.read_csv(file_path)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# forest_fires_df = forest_fires_df.dropna()\n",
    "\n",
    "# # Log transform the 'area' to handle skewness\n",
    "# forest_fires_df['log_area'] = np.log1p(forest_fires_df['area'])\n",
    "\n",
    "# # Select only the most important features\n",
    "# important_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp']\n",
    "\n",
    "# # Apply log transformation to skewed features\n",
    "# for feature in important_features:\n",
    "#     if forest_fires_df[feature].skew() > 1:\n",
    "#         forest_fires_df[feature] = np.log1p(forest_fires_df[feature])\n",
    "\n",
    "# # Filter the dataset to include only important features\n",
    "# X = forest_fires_df[important_features]\n",
    "# y = forest_fires_df['log_area']\n",
    "\n",
    "# # Remove outliers using Z-score\n",
    "# z_scores = np.abs(stats.zscore(X))\n",
    "# X = X[(z_scores < 3).all(axis=1)]\n",
    "# y = y[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Identify continuous features for standardization and interaction\n",
    "# continuous_features = important_features\n",
    "\n",
    "# # Create the column transformer with robust scaler and polynomial features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', RobustScaler(), continuous_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create the polynomial feature generator\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# # Define the pipeline with preprocessing, polynomial features, and Ridge regression\n",
    "# ridge_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('poly', poly),\n",
    "#     ('model', Ridge())\n",
    "# ])\n",
    "\n",
    "# # Define the search space for BayesSearchCV\n",
    "# search_spaces_ridge = {\n",
    "#     'model__alpha': Real(1e-3, 1e+3, prior='log-uniform'),  # Narrowing the search space for better convergence\n",
    "#     'poly__degree': Integer(1, 3)\n",
    "# }\n",
    "\n",
    "# # Initialize BayesSearchCV for Ridge\n",
    "# bayes_search_ridge = BayesSearchCV(\n",
    "#     estimator=ridge_pipeline,\n",
    "#     search_spaces=search_spaces_ridge,\n",
    "#     n_iter=50,\n",
    "#     cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the model with BayesSearchCV\n",
    "# bayes_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model\n",
    "# best_ridge_model = bayes_search_ridge.best_estimator_\n",
    "\n",
    "# # Evaluate Model Performance on Test Data\n",
    "# y_test_pred_ridge = best_ridge_model.predict(X_test)\n",
    "# test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "# test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "# test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "# print(f'Test RMSE (Ridge with BayesSearchCV): {test_rmse_ridge}')\n",
    "# print(f'Test MAE (Ridge with BayesSearchCV): {test_mae_ridge}')\n",
    "# print(f'Test R² (Ridge with BayesSearchCV): {test_r2_ridge}')\n",
    "# print(f'Best Parameters for Ridge: {bayes_search_ridge.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from scipy import stats\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'forestfires.csv'\n",
    "# forest_fires_df = pd.read_csv(file_path)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# forest_fires_df = forest_fires_df.dropna()\n",
    "\n",
    "# # Log transform the 'area' to handle skewness\n",
    "# forest_fires_df['log_area'] = np.log1p(forest_fires_df['area'])\n",
    "\n",
    "# # Select only the most important features\n",
    "# important_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp']\n",
    "\n",
    "# # Apply log transformation to skewed features\n",
    "# for feature in important_features:\n",
    "#     if forest_fires_df[feature].skew() > 1:\n",
    "#         forest_fires_df[feature] = np.log1p(forest_fires_df[feature])\n",
    "\n",
    "# # Filter the dataset to include only important features\n",
    "# X = forest_fires_df[important_features]\n",
    "# y = forest_fires_df['log_area']\n",
    "\n",
    "# # Remove outliers using Z-score\n",
    "# z_scores = np.abs(stats.zscore(X))\n",
    "# X = X[(z_scores < 3).all(axis=1)]\n",
    "# y = y[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Identify continuous features for standardization and interaction\n",
    "# continuous_features = important_features\n",
    "\n",
    "# # Create the column transformer with robust scaler and polynomial features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', RobustScaler(), continuous_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create the polynomial feature generator\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# # Define the pipeline\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('poly', poly),\n",
    "#     ('model', Ridge())  # Placeholder, will be replaced in GridSearchCV\n",
    "# ])\n",
    "\n",
    "# # Define the parameter grid for different models\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'model': [Ridge()],\n",
    "#         'model__alpha': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "#         'poly__degree': [1, 2, 3]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [Lasso(max_iter=10000)],\n",
    "#         'model__alpha': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "#         'poly__degree': [1, 2, 3]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [ElasticNet(max_iter=10000)],\n",
    "#         'model__alpha': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "#         'model__l1_ratio': [0.1, 0.5, 0.7, 1.0],\n",
    "#         'poly__degree': [1, 2, 3]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [SVR()],\n",
    "#         'model__C': [0.1, 1, 10, 100],\n",
    "#         'model__epsilon': [0.01, 0.1, 1],\n",
    "#         'poly__degree': [1, 2, 3]\n",
    "#     },\n",
    "#     {\n",
    "#         'model': [RandomForestRegressor()],\n",
    "#         'model__n_estimators': [50, 100, 200],\n",
    "#         'model__max_depth': [None, 10, 20],\n",
    "#         'poly__degree': [1, 2, 3]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit the model with GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate Model Performance on Test Data\n",
    "# y_test_pred = best_model.predict(X_test)\n",
    "# test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "# test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "# test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print(f'Test RMSE (Best Model): {test_rmse}')\n",
    "# print(f'Test MAE (Best Model): {test_mae}')\n",
    "# print(f'Test R² (Best Model): {test_r2}')\n",
    "# print(f'Best Parameters: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from skopt import BayesSearchCV\n",
    "# from skopt.space import Real, Integer\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from scipy import stats\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'forestfires.csv'\n",
    "# forest_fires_df = pd.read_csv(file_path)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# forest_fires_df = forest_fires_df.dropna()\n",
    "\n",
    "# # Log transform the 'area' to handle skewness\n",
    "# forest_fires_df['log_area'] = np.log1p(forest_fires_df['area'])\n",
    "\n",
    "# # Select only the most important features\n",
    "# important_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp']\n",
    "\n",
    "# # Apply log transformation to skewed features\n",
    "# for feature in important_features:\n",
    "#     if forest_fires_df[feature].skew() > 1:\n",
    "#         forest_fires_df[feature] = np.log1p(forest_fires_df[feature])\n",
    "\n",
    "# # Filter the dataset to include only important features\n",
    "# X = forest_fires_df[important_features]\n",
    "# y = forest_fires_df['log_area']\n",
    "\n",
    "# # Remove outliers using Z-score\n",
    "# z_scores = np.abs(stats.zscore(X))\n",
    "# X = X[(z_scores < 3).all(axis=1)]\n",
    "# y = y[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Identify continuous features for standardization and interaction\n",
    "# continuous_features = important_features\n",
    "\n",
    "# # Create the column transformer with robust scaler and polynomial features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', RobustScaler(), continuous_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Define the pipeline with preprocessing and Gradient Boosting Regressor\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "#     ('model', GradientBoostingRegressor())\n",
    "# ])\n",
    "\n",
    "# # Define the search space for BayesSearchCV\n",
    "# search_spaces = {\n",
    "#     'poly__degree': Integer(1, 3),\n",
    "#     'model__n_estimators': Integer(50, 300),\n",
    "#     'model__learning_rate': Real(0.01, 0.2),\n",
    "#     'model__max_depth': Integer(3, 10),\n",
    "#     'model__subsample': Real(0.6, 1.0),\n",
    "#     'model__min_samples_split': Integer(2, 20),\n",
    "#     'model__min_samples_leaf': Integer(1, 20)\n",
    "# }\n",
    "\n",
    "# # Initialize BayesSearchCV for Gradient Boosting Regressor\n",
    "# bayes_search = BayesSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     search_spaces=search_spaces,\n",
    "#     n_iter=50,\n",
    "#     cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the model with BayesSearchCV\n",
    "# bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model\n",
    "# best_model = bayes_search.best_estimator_\n",
    "\n",
    "# # Evaluate Model Performance on Test Data\n",
    "# y_test_pred = best_model.predict(X_test)\n",
    "# test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "# test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "# test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print(f'Test RMSE (Gradient Boosting with BayesSearchCV): {test_rmse}')\n",
    "# print(f'Test MAE (Gradient Boosting with BayesSearchCV): {test_mae}')\n",
    "# print(f'Test R² (Gradient Boosting with BayesSearchCV): {test_r2}')\n",
    "# print(f'Best Parameters for Gradient Boosting: {bayes_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from skopt import BayesSearchCV\n",
    "# from skopt.space import Real, Integer\n",
    "# from scipy import stats\n",
    "\n",
    "# # Load the data\n",
    "# file_path = 'forestfires.csv'\n",
    "# forest_fires_df = pd.read_csv(file_path)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# forest_fires_df = forest_fires_df.dropna()\n",
    "\n",
    "# # Log transform the 'area' to handle skewness\n",
    "# forest_fires_df['log_area'] = np.log1p(forest_fires_df['area'])\n",
    "\n",
    "# # Select only the most important features\n",
    "# important_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp']\n",
    "\n",
    "# # Apply log transformation to skewed features\n",
    "# for feature in important_features:\n",
    "#     if forest_fires_df[feature].skew() > 1:\n",
    "#         forest_fires_df[feature] = np.log1p(forest_fires_df[feature])\n",
    "\n",
    "# # Filter the dataset to include only important features\n",
    "# X = forest_fires_df[important_features]\n",
    "# y = forest_fires_df['log_area']\n",
    "\n",
    "# # Remove outliers using Z-score\n",
    "# z_scores = np.abs(stats.zscore(X))\n",
    "# X = X[(z_scores < 3).all(axis=1)]\n",
    "# y = y[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Identify continuous features for standardization and interaction\n",
    "# continuous_features = important_features\n",
    "\n",
    "# # Create the column transformer with robust scaler and polynomial features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', RobustScaler(), continuous_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create the polynomial feature generator\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# # Define the pipeline with preprocessing, polynomial features, and Ridge regression\n",
    "# ridge_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('poly', poly),\n",
    "#     ('model', Ridge())\n",
    "# ])\n",
    "\n",
    "# # Define the search space for BayesSearchCV\n",
    "# search_spaces_ridge = {\n",
    "#     'model__alpha': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "#     'poly__degree': Integer(1, 3)  # Also tune polynomial degree\n",
    "# }\n",
    "\n",
    "# # Initialize BayesSearchCV for Ridge\n",
    "# bayes_search_ridge = BayesSearchCV(\n",
    "#     estimator=ridge_pipeline,\n",
    "#     search_spaces=search_spaces_ridge,\n",
    "#     n_iter=50,\n",
    "#     cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the model with BayesSearchCV\n",
    "# bayes_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model\n",
    "# best_ridge_model = bayes_search_ridge.best_estimator_\n",
    "\n",
    "# # Evaluate Model Performance on Test Data\n",
    "# y_test_pred_ridge = best_ridge_model.predict(X_test)\n",
    "# test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "# test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "# test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "# print(f'Test RMSE (Ridge with BayesSearchCV): {test_rmse_ridge}')\n",
    "# print(f'Test MAE (Ridge with BayesSearchCV): {test_mae_ridge}')\n",
    "# print(f'Test R² (Ridge with BayesSearchCV): {test_r2_ridge}')\n",
    "# print(f'Best Parameters for Ridge: {bayes_search_ridge.best_params_}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "forest_fires_df = pd.read_csv('forestfires.csv')\n",
    "\n",
    "# Handle missing values (if any)\n",
    "forest_fires_df = forest_fires_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform the 'area' to handle skewness\n",
    "forest_fires_df['log_area'] = np.log1p(forest_fires_df['area'])\n",
    "\n",
    "# Define columns to be dropped\n",
    "columns_to_drop = ['day', 'area', 'rain', 'X', 'Y', 'month']\n",
    "forest_fires_df = forest_fires_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = forest_fires_df.drop(['log_area'], axis=1)\n",
    "y = forest_fires_df['log_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "q1 = y.quantile(0.25)\n",
    "q3 = y.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "X = X[(y >= lower_bound) & (y <= upper_bound)]\n",
    "y = y[(y >= lower_bound) & (y <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous features for standardization\n",
    "continuous_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column transformer with standard scaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # leave the rest of the columns unchanged\n",
    ")\n",
    "\n",
    "# Add polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the preprocessor, polynomial features, and XGBoost model\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', poly),\n",
    "    ('model', XGBRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Test RMSE (XGBoost with RandomizedSearchCV): 1.2997128091199317\n",
      "Test MAE (XGBoost with RandomizedSearchCV): 1.0934636921681715\n",
      "Test R² (XGBoost with RandomizedSearchCV): -0.030981559691388938\n",
      "Best Parameters (XGBoost with RandomizedSearchCV): {'model__colsample_bytree': 0.8834959481464842, 'model__learning_rate': 0.011413261043943482, 'model__max_depth': 3, 'model__n_estimators': 148, 'model__subsample': 0.8574323980775167}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter distribution for randomized search\n",
    "param_dist = {\n",
    "    'model__n_estimators': randint(100, 300),\n",
    "    'model__max_depth': randint(3, 10),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__subsample': uniform(0.7, 0.3),\n",
    "    'model__colsample_bytree': uniform(0.7, 0.3)\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with a more robust cross-validation strategy\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=xgb_pipeline, param_distributions=param_dist, n_iter=100, cv=kf, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit the model with RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate Model Performance on Test Data\n",
    "y_test_pred_xgb = best_xgb_model.predict(X_test)\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f'Test RMSE (XGBoost with RandomizedSearchCV): {test_rmse_xgb}')\n",
    "print(f'Test MAE (XGBoost with RandomizedSearchCV): {test_mae_xgb}')\n",
    "print(f'Test R² (XGBoost with RandomizedSearchCV): {test_r2_xgb}')\n",
    "print(f'Best Parameters (XGBoost with RandomizedSearchCV): {random_search.best_params_}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
